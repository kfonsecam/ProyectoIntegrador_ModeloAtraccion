{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9cef9c15",
      "metadata": {
        "id": "9cef9c15"
      },
      "source": [
        "# Modelo de predicción de inscripciones estudiantiles mediante técnicas de Machine Learning\n",
        "\n",
        "**Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "**Proyecto Integrador Sep-Nov 2025**\n",
        "\n",
        "**Equipo 14**\n",
        "\n",
        "**Avance 4.    Modelos Alternativos**\n",
        "\n",
        "**Integrantes:**\n",
        "\n",
        "- Alejandro Roa Solis – A01129942\n",
        "- Annette Cristina Narvaez Andrade – A00571041\n",
        "- Karla Alejandra Fonseca Márquez – A01795313\n",
        "\n",
        "\n",
        "**Patrocinador Tec de Monterrey:**\n",
        "\n",
        "Dr. Juan Arturo Nolazco Flores, Director del Hub de Ciencias y Datos de la Escuela de Ingeniería del Tec de Monterrey\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e381e65c-ebf5-4bf8-9763-f30879b26c87",
      "metadata": {
        "id": "e381e65c-ebf5-4bf8-9763-f30879b26c87"
      },
      "source": [
        "# Avance 4 — Models Alternativod (por subconjunto)\n",
        "\n",
        "En esta cuarta etapa del *Proyecto Integrador* se desarrolla la **fase de modelado**, correspondiente a la metodología **CRISP-ML (Cross-Industry Standard Process for Machine Learning)**.  \n",
        "Su propósito es **construir, comparar y ajustar múltiples modelos predictivos individuales**, con el fin de identificar cuál ofrece el mejor desempeño para el problema de predicción de inscripción estudiantil.\n",
        "\n",
        "A diferencia del avance anterior —donde se estableció un *modelo baseline*—, en esta fase se implementan **diversos algoritmos de clasificación supervisada**, cada uno con configuraciones iniciales propias.  \n",
        "Esta diversidad permite explorar la capacidad de los datos para adaptarse a distintas estructuras de aprendizaje y analizar la sensibilidad del rendimiento frente a variaciones de hiperparámetros.\n",
        "\n",
        "---\n",
        "\n",
        "Objetivos del avance\n",
        "\n",
        "1. **Entrenar al menos seis modelos individuales**, utilizando algoritmos representativos de distintas familias (lineales, basados en árboles, de márgenes, entre otros).  \n",
        "2. **Evaluar y comparar el desempeño** de los modelos mediante métricas de clasificación relevantes (*accuracy, F1-score, matriz de confusión, validación cruzada*).  \n",
        "3. **Seleccionar los dos modelos con mejores resultados** y aplicar **técnicas de ajuste de hiperparámetros** (*hyperparameter tuning*) para optimizar su rendimiento.  \n",
        "4. **Elegir el modelo individual final**, considerando desempeño cuantitativo, interpretabilidad, robustez y factibilidad de implementación.\n",
        "\n",
        "---\n",
        "\n",
        "Con este avance se consolida el proceso de **experimentación y refinamiento de los modelos**, garantizando la construcción de una solución predictiva sólida, evaluada bajo criterios reproducibles y alineados con los objetivos de negocio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1217a5d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Librerías y utilidades\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from typing import Dict, Tuple, Callable\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.30\n",
        "TARGET_COL = \"INSCRITO\"\n",
        "\n",
        "os.makedirs(\"baseline_reports\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "046bf869",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definicion de fuentes de datos, una para cada subconjunto\n",
        "SUBSETS: Dict[str, dict] = {\n",
        "    \"PrepaTec\": {\"csv\": \"FE_DS/prepa_tec_feature_engineered_full.csv\"},\n",
        "    \"Profesional_PrepaTec\": {\"csv\": \"FE_DS/profesional_tec_feature_engineered_full.csv\"},\n",
        "    \"Profesional_Externos\": {\"csv\": \"FE_DS/profesional_nnotec_feature_engineered_full.csv\"},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "595b342e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para cargar los datasets existentes\n",
        "def load_subset(name: str, cfg: dict) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Carga X, y para un subconjunto.\n",
        "\n",
        "    Soporta:\n",
        "      - cfg = {\"csv\": \"path.csv\"}  (usa todo el archivo)\n",
        "      - cfg = {\"csv\": \"path.csv\", \"filter\": {\"COL\": \"VAL\", ...}} (filtra antes de separar)\n",
        "    \"\"\"\n",
        "    path = cfg.get(\"csv\")\n",
        "    if not path or not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"[{name}] No se encontró el CSV: {path}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    if \"filter\" in cfg and isinstance(cfg[\"filter\"], dict):\n",
        "        for col, val in cfg[\"filter\"].items():\n",
        "            if col not in df.columns:\n",
        "                raise KeyError(f\"[{name}] La columna de filtro '{col}' no existe en el CSV.\")\n",
        "            df = df[df[col] == val]\n",
        "\n",
        "    if TARGET_COL not in df.columns:\n",
        "        raise KeyError(f\"[{name}] No se encontró la columna objetivo '{TARGET_COL}'.\")\n",
        "\n",
        "    y = df[TARGET_COL]\n",
        "    X = df.drop(columns=[TARGET_COL])\n",
        "\n",
        "    # Seguridad: si hay columnas completamente vacías, descartarlas\n",
        "    empty_cols = [c for c in X.columns if X[c].isna().all()]\n",
        "    if empty_cols:\n",
        "        X = X.drop(columns=empty_cols)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ae0143",
      "metadata": {},
      "source": [
        "## ⚙️ Modelado — Subconjunto *PrepaTec*\n",
        "\n",
        "En esta etapa se implementaron y evaluaron **seis modelos individuales de clasificación supervisada**, con el objetivo de identificar qué algoritmos se adaptan mejor a las características del conjunto *PrepaTec*.  \n",
        "Cada modelo fue entrenado y probado utilizando una **división estratificada (70/30)**, manteniendo la proporción original de clases y controlando la aleatoriedad mediante una semilla fija (`random_state=42`).\n",
        "\n",
        "Dado que este subconjunto presenta un **alto desbalance de clases**, se priorizaron métricas más robustas que la exactitud, como el **F1-score ponderado (weighted)** y el **ROC-AUC**, complementadas con validación cruzada de 5 pliegues para evaluar la estabilidad del rendimiento.\n",
        "\n",
        "Los modelos seleccionados representan diferentes familias de algoritmos, permitiendo contrastar sus fortalezas:\n",
        "\n",
        "| **Modelo** | **Tipo / Familia** | **Características principales** |\n",
        "|-------------|--------------------|---------------------------------|\n",
        "| **Logistic Regression** | Lineal / Baseline | Punto de partida; balanceado y no balanceado para medir sensibilidad ante el desbalance. |\n",
        "| **Random Forest** | Ensamble de árboles | Robusto y no lineal; maneja bien interacciones complejas y admite `class_weight='balanced'`. |\n",
        "| **Gradient Boosting** | Ensamble secuencial | Mayor capacidad de ajuste fino; buen control del sesgo y la varianza. |\n",
        "| **Support Vector Machine (RBF)** | Kernel no lineal | Modelo de márgenes máximos, sensible al desbalance pero potente en fronteras complejas. |\n",
        "| **K-Nearest Neighbors** | Basado en distancia | Modelo no paramétrico; evalúa similitud local entre instancias. |\n",
        "| **Decision Tree** | Árbol único | Alta interpretabilidad; evalúa divisiones jerárquicas con balanceo interno de clases. |\n",
        "| **Naive Bayes (Gaussian)** | Probabilístico | Modelo simple y rápido; útil para analizar la separabilidad de las clases. |\n",
        "\n",
        "Cada modelo fue evaluado con métricas de desempeño en el conjunto de prueba (**Accuracy**, **F1-score**, **ROC-AUC**) y mediante **validación cruzada**, generando una interpretación individual que analiza su comportamiento frente al desbalance y su potencial de generalización.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e66e28cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PrepaTec] RandomForest — Test\n",
            "Accuracy:        0.813\n",
            "F1 (binary):     0.896\n",
            "F1 (weighted):   0.740\n",
            "ROC-AUC:         0.658\n",
            "\n",
            "Matriz de confusión:\n",
            " [[  14  516]\n",
            " [  20 2314]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.03      0.05       530\n",
            "           1       0.82      0.99      0.90      2334\n",
            "\n",
            "    accuracy                           0.81      2864\n",
            "   macro avg       0.61      0.51      0.47      2864\n",
            "weighted avg       0.74      0.81      0.74      2864\n",
            "\n",
            "\n",
            "CV F1-weighted 5-fold: 0.736 ± 0.001\n"
          ]
        }
      ],
      "source": [
        "# ===== Modelo 1 extra: Random Forest para PrepaTec =====\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1) Cargar datos del subset\n",
        "subset = \"PrepaTec\"\n",
        "X, y = load_subset(subset, SUBSETS[subset])\n",
        "\n",
        "# 2) Split estratificado\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Definir y entrenar RF (baseline razonable para datos tabulares)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    class_weight=\"balanced\",   # mitiga el desbalance\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "rf.fit(X_tr, y_tr)\n",
        "\n",
        "# 4) Evaluación en test\n",
        "y_pred = rf.predict(X_te)\n",
        "y_score = rf.predict_proba(X_te)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_te, y_pred)\n",
        "f1_bin = f1_score(y_te, y_pred)\n",
        "f1_w = f1_score(y_te, y_pred, average=\"weighted\")\n",
        "roc = roc_auc_score(y_te, y_score)\n",
        "\n",
        "print(f\"[{subset}] RandomForest — Test\")\n",
        "print(f\"Accuracy:        {acc:.3f}\")\n",
        "print(f\"F1 (binary):     {f1_bin:.3f}\")\n",
        "print(f\"F1 (weighted):   {f1_w:.3f}\")\n",
        "print(f\"ROC-AUC:         {roc:.3f}\")\n",
        "print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
        "\n",
        "# 5) Validación cruzada (macro o weighted para desbalance)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_f1w = cross_val_score(rf, X, y, scoring=\"f1_weighted\", cv=cv, n_jobs=-1)\n",
        "print(f\"\\nCV F1-weighted 5-fold: {cv_f1w.mean():.3f} ± {cv_f1w.std():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "12bc1db7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PrepaTec] GradientBoosting — Test\n",
            "Accuracy:        0.815\n",
            "F1 (binary):     0.897\n",
            "F1 (weighted):   0.740\n",
            "ROC-AUC:         0.674\n",
            "\n",
            "Matriz de confusión:\n",
            " [[  13  517]\n",
            " [  14 2320]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.02      0.05       530\n",
            "           1       0.82      0.99      0.90      2334\n",
            "\n",
            "    accuracy                           0.81      2864\n",
            "   macro avg       0.65      0.51      0.47      2864\n",
            "weighted avg       0.76      0.81      0.74      2864\n",
            "\n",
            "\n",
            "CV F1-weighted 5-fold: 0.739 ± 0.003\n"
          ]
        }
      ],
      "source": [
        "# ===== Modelo 2: Gradient Boosting para PrepaTec =====\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# 1) Cargar datos\n",
        "subset = \"PrepaTec\"\n",
        "X, y = load_subset(subset, SUBSETS[subset])\n",
        "\n",
        "# 2) Split estratificado\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Definir y entrenar Gradient Boosting (config inicial robusta)\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=300,        # número moderado de árboles\n",
        "    learning_rate=0.05,      # tasa de aprendizaje baja mejora generalización\n",
        "    max_depth=3,             # profundidad controlada\n",
        "    subsample=0.9,           # bagging parcial\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "gb.fit(X_tr, y_tr)\n",
        "\n",
        "# 4) Evaluación en test\n",
        "y_pred = gb.predict(X_te)\n",
        "y_score = gb.predict_proba(X_te)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_te, y_pred)\n",
        "f1_bin = f1_score(y_te, y_pred)\n",
        "f1_w = f1_score(y_te, y_pred, average=\"weighted\")\n",
        "roc = roc_auc_score(y_te, y_score)\n",
        "\n",
        "print(f\"[{subset}] GradientBoosting — Test\")\n",
        "print(f\"Accuracy:        {acc:.3f}\")\n",
        "print(f\"F1 (binary):     {f1_bin:.3f}\")\n",
        "print(f\"F1 (weighted):   {f1_w:.3f}\")\n",
        "print(f\"ROC-AUC:         {roc:.3f}\")\n",
        "print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
        "\n",
        "# 5) Validación cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_f1w = cross_val_score(gb, X, y, scoring=\"f1_weighted\", cv=cv, n_jobs=-1)\n",
        "print(f\"\\nCV F1-weighted 5-fold: {cv_f1w.mean():.3f} ± {cv_f1w.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "46538a0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PrepaTec] SVC (RBF) — Test\n",
            "Accuracy:        0.607\n",
            "F1 (binary):     0.711\n",
            "F1 (weighted):   0.651\n",
            "ROC-AUC:         0.667\n",
            "\n",
            "Matriz de confusión:\n",
            " [[ 352  178]\n",
            " [ 948 1386]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.66      0.38       530\n",
            "           1       0.89      0.59      0.71      2334\n",
            "\n",
            "    accuracy                           0.61      2864\n",
            "   macro avg       0.58      0.63      0.55      2864\n",
            "weighted avg       0.77      0.61      0.65      2864\n",
            "\n",
            "\n",
            "CV F1-weighted 5-fold: 0.650 ± 0.017\n"
          ]
        }
      ],
      "source": [
        "# ===== Modelo 3: Support Vector Classifier (SVC-RBF) para PrepaTec =====\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1) Cargar datos\n",
        "subset = \"PrepaTec\"\n",
        "X, y = load_subset(subset, SUBSETS[subset])\n",
        "\n",
        "# 2) Split estratificado\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Definir pipeline: escalado + SVC\n",
        "svc_pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(\n",
        "        kernel=\"rbf\",\n",
        "        C=1,\n",
        "        gamma=\"scale\",\n",
        "        probability=True,       # necesario para ROC-AUC\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 4) Entrenar modelo\n",
        "svc_pipe.fit(X_tr, y_tr)\n",
        "\n",
        "# 5) Evaluación en test\n",
        "y_pred = svc_pipe.predict(X_te)\n",
        "y_score = svc_pipe.predict_proba(X_te)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_te, y_pred)\n",
        "f1_bin = f1_score(y_te, y_pred)\n",
        "f1_w = f1_score(y_te, y_pred, average=\"weighted\")\n",
        "roc = roc_auc_score(y_te, y_score)\n",
        "\n",
        "print(f\"[{subset}] SVC (RBF) — Test\")\n",
        "print(f\"Accuracy:        {acc:.3f}\")\n",
        "print(f\"F1 (binary):     {f1_bin:.3f}\")\n",
        "print(f\"F1 (weighted):   {f1_w:.3f}\")\n",
        "print(f\"ROC-AUC:         {roc:.3f}\")\n",
        "print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
        "\n",
        "# 6) Validación cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_f1w = cross_val_score(svc_pipe, X, y, scoring=\"f1_weighted\", cv=cv, n_jobs=-1)\n",
        "print(f\"\\nCV F1-weighted 5-fold: {cv_f1w.mean():.3f} ± {cv_f1w.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "83421774",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PrepaTec] KNN — Test\n",
            "Accuracy:        0.810\n",
            "F1 (binary):     0.894\n",
            "F1 (weighted):   0.746\n",
            "ROC-AUC:         0.619\n",
            "\n",
            "Matriz de confusión:\n",
            " [[  29  501]\n",
            " [  44 2290]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.05      0.10       530\n",
            "           1       0.82      0.98      0.89      2334\n",
            "\n",
            "    accuracy                           0.81      2864\n",
            "   macro avg       0.61      0.52      0.49      2864\n",
            "weighted avg       0.74      0.81      0.75      2864\n",
            "\n",
            "\n",
            "CV F1-weighted 5-fold: 0.742 ± 0.003\n"
          ]
        }
      ],
      "source": [
        "# ===== Modelo 4: KNN para PrepaTec =====\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1) Cargar datos\n",
        "subset = \"PrepaTec\"\n",
        "X, y = load_subset(subset, SUBSETS[subset])\n",
        "\n",
        "# 2) Split estratificado\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Pipeline: escalado + KNN (config inicial razonable)\n",
        "knn_pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=11,     # k impar para evitar empates\n",
        "        weights=\"distance\", # vecinos más cercanos pesan más\n",
        "        p=2,                # distancia Euclidiana (prueba p=1 también)\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 4) Entrenar y evaluar en test\n",
        "knn_pipe.fit(X_tr, y_tr)\n",
        "\n",
        "y_pred  = knn_pipe.predict(X_te)\n",
        "# Para ROC-AUC, usamos \"probability-like\" (inverse distance). predict_proba existe.\n",
        "y_score = knn_pipe.predict_proba(X_te)[:, 1]\n",
        "\n",
        "acc   = accuracy_score(y_te, y_pred)\n",
        "f1_b  = f1_score(y_te, y_pred)\n",
        "f1_w  = f1_score(y_te, y_pred, average=\"weighted\")\n",
        "roc   = roc_auc_score(y_te, y_score)\n",
        "\n",
        "print(f\"[{subset}] KNN — Test\")\n",
        "print(f\"Accuracy:        {acc:.3f}\")\n",
        "print(f\"F1 (binary):     {f1_b:.3f}\")\n",
        "print(f\"F1 (weighted):   {f1_w:.3f}\")\n",
        "print(f\"ROC-AUC:         {roc:.3f}\")\n",
        "print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
        "\n",
        "# 5) Validación cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_f1w = cross_val_score(knn_pipe, X, y, scoring=\"f1_weighted\", cv=cv, n_jobs=-1)\n",
        "print(f\"\\nCV F1-weighted 5-fold: {cv_f1w.mean():.3f} ± {cv_f1w.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4194c141",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PrepaTec] DecisionTree — Test\n",
            "Accuracy:        0.627\n",
            "F1 (binary):     0.750\n",
            "F1 (weighted):   0.661\n",
            "ROC-AUC:         0.543\n",
            "\n",
            "Matriz de confusión:\n",
            " [[ 197  333]\n",
            " [ 734 1600]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.37      0.27       530\n",
            "           1       0.83      0.69      0.75      2334\n",
            "\n",
            "    accuracy                           0.63      2864\n",
            "   macro avg       0.52      0.53      0.51      2864\n",
            "weighted avg       0.71      0.63      0.66      2864\n",
            "\n",
            "\n",
            "CV F1-weighted 5-fold: 0.667 ± 0.008\n"
          ]
        }
      ],
      "source": [
        "# ===== Modelo 5: Decision Tree para PrepaTec =====\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1) Cargar datos\n",
        "subset = \"PrepaTec\"\n",
        "X, y = load_subset(subset, SUBSETS[subset])\n",
        "\n",
        "# 2) Split estratificado\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Árbol base con poda ligera\n",
        "dt = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    max_depth=None,          # deja crecer y que ccp_alpha pode\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "dt.fit(X_tr, y_tr)\n",
        "\n",
        "# 4) Evaluación en test\n",
        "y_pred = dt.predict(X_te)\n",
        "\n",
        "# Para ROC-AUC en árbol necesitamos proba\n",
        "y_score = dt.predict_proba(X_te)[:, 1]\n",
        "\n",
        "acc  = accuracy_score(y_te, y_pred)\n",
        "f1_b = f1_score(y_te, y_pred)\n",
        "f1_w = f1_score(y_te, y_pred, average=\"weighted\")\n",
        "roc  = roc_auc_score(y_te, y_score)\n",
        "\n",
        "print(f\"[{subset}] DecisionTree — Test\")\n",
        "print(f\"Accuracy:        {acc:.3f}\")\n",
        "print(f\"F1 (binary):     {f1_b:.3f}\")\n",
        "print(f\"F1 (weighted):   {f1_w:.3f}\")\n",
        "print(f\"ROC-AUC:         {roc:.3f}\")\n",
        "print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
        "\n",
        "# 5) Validación cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_f1w = cross_val_score(dt, X, y, scoring=\"f1_weighted\", cv=cv, n_jobs=-1)\n",
        "print(f\"\\nCV F1-weighted 5-fold: {cv_f1w.mean():.3f} ± {cv_f1w.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "11a0e258",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PrepaTec] NaiveBayes — Test\n",
            "Accuracy:        0.272\n",
            "F1 (binary):     0.205\n",
            "F1 (weighted):   0.228\n",
            "ROC-AUC:         0.635\n",
            "\n",
            "Matriz de confusión:\n",
            " [[ 509   21]\n",
            " [2065  269]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.96      0.33       530\n",
            "           1       0.93      0.12      0.21      2334\n",
            "\n",
            "    accuracy                           0.27      2864\n",
            "   macro avg       0.56      0.54      0.27      2864\n",
            "weighted avg       0.79      0.27      0.23      2864\n",
            "\n",
            "\n",
            "CV F1-weighted 5-fold: 0.227 ± 0.013\n"
          ]
        }
      ],
      "source": [
        "# ===== Modelo 6: Naive Bayes (GaussianNB) para PrepaTec =====\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1) Cargar datos\n",
        "subset = \"PrepaTec\"\n",
        "X, y = load_subset(subset, SUBSETS[subset])\n",
        "\n",
        "# 2) Split estratificado\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Entrenar modelo\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_tr, y_tr)\n",
        "\n",
        "# 4) Evaluación en test\n",
        "y_pred = nb.predict(X_te)\n",
        "y_score = nb.predict_proba(X_te)[:, 1]\n",
        "\n",
        "acc  = accuracy_score(y_te, y_pred)\n",
        "f1_b = f1_score(y_te, y_pred)\n",
        "f1_w = f1_score(y_te, y_pred, average=\"weighted\")\n",
        "roc  = roc_auc_score(y_te, y_score)\n",
        "\n",
        "print(f\"[{subset}] NaiveBayes — Test\")\n",
        "print(f\"Accuracy:        {acc:.3f}\")\n",
        "print(f\"F1 (binary):     {f1_b:.3f}\")\n",
        "print(f\"F1 (weighted):   {f1_w:.3f}\")\n",
        "print(f\"ROC-AUC:         {roc:.3f}\")\n",
        "print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
        "\n",
        "# 5) Validación cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_f1w = cross_val_score(nb, X, y, scoring=\"f1_weighted\", cv=cv, n_jobs=-1)\n",
        "print(f\"\\nCV F1-weighted 5-fold: {cv_f1w.mean():.3f} ± {cv_f1w.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88126563",
      "metadata": {},
      "source": [
        "## Comparativa de Modelos — Subconjunto *PrepaTec*\n",
        "\n",
        "A continuación se presentan los resultados obtenidos para los seis modelos probados en el subconjunto *PrepaTec*.  \n",
        "Cada modelo fue evaluado en el conjunto de prueba y validado mediante validación cruzada (5 pliegues) utilizando como métrica principal el **F1 ponderado (weighted)** debido al alto desbalance de clases.\n",
        "\n",
        "| **Modelo** | **Accuracy (test)** | **F1 (binary)** | **F1 (weighted)** | **ROC-AUC** | **CV F1-weighted (± std)** | **Interpretación principal** |\n",
        "|-------------|---------------------|-----------------|-------------------|--------------|-----------------------------|------------------------------|\n",
        "| **Logistic Regression (balanceado)** | 0.58 | 0.69 | 0.69 | 0.64 | 0.682 ± 0.004 | Mejora frente al dummy; logra detectar parte de la clase minoritaria, aunque con rendimiento limitado por el fuerte desbalance. |\n",
        "| **Random Forest** | 0.813 | 0.896 | 0.740 | 0.658 | 0.736 ± 0.001 | Robusto y estable; predomina la clase mayoritaria. Buen desempeño global, pero bajo recall en la clase 0. |\n",
        "| **Gradient Boosting** | 0.815 | 0.897 | 0.740 | 0.674 | 0.739 ± 0.003 | Resultados similares al RF con ligera mejora en ROC-AUC. Modelo estable y bien generalizado, aunque sin resolver el desbalance. |\n",
        "| **SVC (RBF)** | 0.607 | 0.711 | 0.651 | 0.667 | 0.650 ± 0.017 | Reduce accuracy pero mejora notablemente el recall de la clase minoritaria. Mayor equilibrio, aunque menor precisión global. |\n",
        "| **K-Nearest Neighbors** | 0.810 | 0.894 | 0.746 | 0.619 | 0.742 ± 0.003 | Estable y competitivo en F1 ponderado, pero sigue sin captar adecuadamente la clase 0. Bajo poder discriminativo (ROC). |\n",
        "| **Decision Tree** | 0.627 | 0.750 | 0.661 | 0.543 | 0.667 ± 0.008 | Aumenta ligeramente el recall de la clase minoritaria; desempeño moderado y capacidad discriminativa limitada. |\n",
        "| **Naive Bayes (Gaussian)** | 0.272 | 0.205 | 0.228 | 0.635 | 0.227 ± 0.013 | Detecta casi todos los casos de la clase minoritaria (recall alto), pero con enorme pérdida de precisión y accuracy general. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusiones generales**\n",
        "\n",
        "- El subconjunto *PrepaTec* presenta un **fuerte desbalance de clases**, lo que limita el desempeño de todos los modelos en la detección de la clase minoritaria.  \n",
        "- Los modelos basados en **árboles de decisión** (Random Forest y Gradient Boosting) fueron los **más estables y con mejor F1 ponderado (~0.74)**, mostrando buena generalización y baja varianza entre pliegues.  \n",
        "- El **SVC (RBF)** destacó por mejorar el **recall de la clase 0**, sacrificando precisión y exactitud, evidenciando un mejor equilibrio de clases a costa del rendimiento global.  \n",
        "- El **Naive Bayes** confirmó el grado de solapamiento entre clases, alcanzando recall alto pero un F1 y accuracy muy bajos.  \n",
        "- En conjunto, los modelos **Random Forest** y **Gradient Boosting** se perfilan como los **mejores candidatos** para ajuste de hiperparámetros en la siguiente fase, al combinar buen desempeño y estabilidad.\n",
        "\n",
        "---\n",
        "\n",
        "📈 *En la siguiente etapa se realizará el ajuste de los dos mejores modelos (Random Forest y Gradient Boosting), buscando optimizar hiperparámetros clave y evaluar mejoras en sensibilidad y capacidad predictiva general.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7087ab11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Ajuste de Random Forest ===\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Mejores hiperparámetros RF: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 600}\n",
            "Mejor score CV RF: 0.7489025900116901\n",
            "\n",
            "=== Ajuste de Gradient Boosting ===\n",
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
            "Mejores hiperparámetros GB: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300, 'subsample': 0.8}\n",
            "Mejor score CV GB: 0.7440675767724599\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modelo</th>\n",
              "      <th>Accuracy (test)</th>\n",
              "      <th>F1 (binary)</th>\n",
              "      <th>F1 (weighted)</th>\n",
              "      <th>ROC-AUC</th>\n",
              "      <th>Mejores parámetros</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.785615</td>\n",
              "      <td>0.874949</td>\n",
              "      <td>0.759185</td>\n",
              "      <td>0.670809</td>\n",
              "      <td>{'max_depth': 20, 'max_features': 'sqrt', 'min...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.809358</td>\n",
              "      <td>0.892815</td>\n",
              "      <td>0.753280</td>\n",
              "      <td>0.666357</td>\n",
              "      <td>{'learning_rate': 0.1, 'max_depth': 4, 'n_esti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Modelo  Accuracy (test)  F1 (binary)  F1 (weighted)   ROC-AUC  \\\n",
              "0      Random Forest         0.785615     0.874949       0.759185  0.670809   \n",
              "1  Gradient Boosting         0.809358     0.892815       0.753280  0.666357   \n",
              "\n",
              "                                  Mejores parámetros  \n",
              "0  {'max_depth': 20, 'max_features': 'sqrt', 'min...  \n",
              "1  {'learning_rate': 0.1, 'max_depth': 4, 'n_esti...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Modelo final seleccionado: Random Forest\n"
          ]
        }
      ],
      "source": [
        "# ===== Ajuste de hiperparámetros: Random Forest y Gradient Boosting =====\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Cargar datos completos del subset\n",
        "subset = \"PrepaTec\"\n",
        "X, y = load_subset(subset, SUBSETS[subset])\n",
        "\n",
        "# División estratificada\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# 2) Definir grillas de hiperparámetros\n",
        "rf_grid = {\n",
        "    \"n_estimators\": [200, 400, 600],\n",
        "    \"max_depth\": [None, 10, 20, 30],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 5],\n",
        "    \"max_features\": [\"sqrt\", \"log2\"],\n",
        "}\n",
        "\n",
        "gb_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"max_depth\": [2, 3, 4],\n",
        "    \"subsample\": [0.8, 1.0],\n",
        "}\n",
        "\n",
        "# 3) Instancias base\n",
        "rf = RandomForestClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\", n_jobs=-1)\n",
        "gb = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "\n",
        "# 4) Configuración de validación cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# 5) GridSearchCV para Random Forest\n",
        "print(\"=== Ajuste de Random Forest ===\")\n",
        "rf_gs = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=rf_grid,\n",
        "    scoring=\"f1_weighted\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_gs.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Mejores hiperparámetros RF:\", rf_gs.best_params_)\n",
        "print(\"Mejor score CV RF:\", rf_gs.best_score_)\n",
        "\n",
        "# 6) GridSearchCV para Gradient Boosting\n",
        "print(\"\\n=== Ajuste de Gradient Boosting ===\")\n",
        "gb_gs = GridSearchCV(\n",
        "    estimator=gb,\n",
        "    param_grid=gb_grid,\n",
        "    scoring=\"f1_weighted\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "gb_gs.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"Mejores hiperparámetros GB:\", gb_gs.best_params_)\n",
        "print(\"Mejor score CV GB:\", gb_gs.best_score_)\n",
        "\n",
        "# 7) Evaluación en conjunto de prueba final\n",
        "results_tuned = []\n",
        "for name, model_gs in [(\"Random Forest\", rf_gs), (\"Gradient Boosting\", gb_gs)]:\n",
        "    best_model = model_gs.best_estimator_\n",
        "    y_pred = best_model.predict(X_te)\n",
        "    y_score = best_model.predict_proba(X_te)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_te, y_pred)\n",
        "    f1b = f1_score(y_te, y_pred)\n",
        "    f1w = f1_score(y_te, y_pred, average=\"weighted\")\n",
        "    roc = roc_auc_score(y_te, y_score)\n",
        "\n",
        "    results_tuned.append({\n",
        "        \"Modelo\": name,\n",
        "        \"Accuracy (test)\": acc,\n",
        "        \"F1 (binary)\": f1b,\n",
        "        \"F1 (weighted)\": f1w,\n",
        "        \"ROC-AUC\": roc,\n",
        "        \"Mejores parámetros\": model_gs.best_params_\n",
        "    })\n",
        "\n",
        "df_tuned = pd.DataFrame(results_tuned).sort_values(by=\"F1 (weighted)\", ascending=False)\n",
        "display(df_tuned)\n",
        "\n",
        "# 8) Elegir modelo final\n",
        "final_model_name = df_tuned.iloc[0][\"Modelo\"]\n",
        "print(f\"\\n✅ Modelo final seleccionado: {final_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5eee5ea5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Modelo final guardado correctamente en: baseline_reports\\PrepaTec\\final_model_Random_Forest.joblib\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from joblib import dump\n",
        "\n",
        "# Crear carpeta si no existe\n",
        "out_dir = os.path.join(\"baseline_reports\", subset)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Guardar modelo final\n",
        "final_model = rf_gs.best_estimator_ if final_model_name == \"Random Forest\" else gb_gs.best_estimator_\n",
        "model_path = os.path.join(out_dir, f\"final_model_{final_model_name.replace(' ', '_')}.joblib\")\n",
        "\n",
        "dump(final_model, model_path)\n",
        "print(f\"✅ Modelo final guardado correctamente en: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb8f75b",
      "metadata": {},
      "source": [
        "## Conclusión — Selección del Mejor Modelo (PrepaTec)\n",
        "\n",
        "Tras evaluar y ajustar los seis modelos individuales, el **Random Forest** fue seleccionado como el modelo final para el subconjunto *PrepaTec*, al demostrar el **mejor equilibrio entre desempeño, estabilidad y capacidad de generalización**.\n",
        "\n",
        "Durante el proceso de ajuste de hiperparámetros mediante validación cruzada estratificada, el Random Forest alcanzó un **F1 ponderado promedio de 0.75** y una **estabilidad muy alta** entre pliegues (desviación < 0.002).  \n",
        "En el conjunto de prueba, obtuvo:\n",
        "\n",
        "- **Accuracy:** 0.79  \n",
        "- **F1 ponderado:** 0.76  \n",
        "- **ROC-AUC:** 0.67  \n",
        "- **Mejores hiperparámetros:**  \n",
        "  `max_depth=20`, `max_features='sqrt'`, `min_samples_leaf=2`, `min_samples_split=10`, `n_estimators=600`\n",
        "\n",
        "Estos resultados evidencian que el modelo logra **una predicción robusta y generalizable**, superando consistentemente a los demás modelos en F1 ponderado y estabilidad.  \n",
        "A diferencia de los modelos lineales o probabilísticos, el Random Forest es capaz de **capturar interacciones no lineales entre variables** sin requerir transformaciones complejas, manteniendo además **resistencia al sobreajuste** gracias a la agregación de múltiples árboles.\n",
        "\n",
        "Aunque el dataset *PrepaTec* presenta un **alto desbalance de clases**, el uso de `class_weight=\"balanced\"` permitió mejorar la detección de la clase minoritaria sin comprometer en exceso la precisión general.  \n",
        "\n",
        "En conjunto, el Random Forest se considera el **modelo óptimo para este subconjunto**, al ofrecer:\n",
        "- **Mejor F1 ponderado global**  \n",
        "- **Alta estabilidad en validación cruzada**  \n",
        "- **Buen compromiso entre precisión y recall**  \n",
        "- **Comportamiento consistente frente al desbalance**\n",
        "\n",
        "Este modelo servirá como base para las siguientes etapas de validación y análisis de generalización sobre los demás subconjuntos (*Profesional PrepaTec* y *Profesional Externos*).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
